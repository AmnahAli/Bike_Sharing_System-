{
 "metadata": {
  "name": "",
  "signature": "sha256:cc8ec64cb62f6ca9627fe2fb72d1a08c86dbc0c85c04c03a6301db7a0f2f3a8b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Loading package and data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import math\n",
      " \n",
      "from sklearn import ensemble\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from datetime import datetime\n",
      " \n",
      "#Load Data with pandas, and parse the\n",
      "#first column into datetime\n",
      "train = pd.read_csv('train.csv', parse_dates=[0])\n",
      "test = pd.read_csv('test.csv', parse_dates=[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Feature engineering"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Feature engineering\n",
      "temp = pd.DatetimeIndex(train['datetime'])\n",
      "train['year'] = temp.year\n",
      "train['month'] = temp.month\n",
      "train['hour'] = temp.hour\n",
      "train['weekday'] = temp.weekday\n",
      " \n",
      "temp = pd.DatetimeIndex(test['datetime'])\n",
      "test['year'] = temp.year\n",
      "test['month'] = temp.month\n",
      "test['hour'] = temp.hour\n",
      "test['weekday'] = temp.weekday\n",
      " \n",
      "#Define features vector\n",
      "features = ['season', 'holiday', 'workingday', 'weather',\n",
      "        'temp', 'atemp', 'humidity', 'windspeed', 'year',\n",
      "         'month', 'weekday', 'hour']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train[features].head(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>season</th>\n",
        "      <th>holiday</th>\n",
        "      <th>workingday</th>\n",
        "      <th>weather</th>\n",
        "      <th>temp</th>\n",
        "      <th>atemp</th>\n",
        "      <th>humidity</th>\n",
        "      <th>windspeed</th>\n",
        "      <th>year</th>\n",
        "      <th>month</th>\n",
        "      <th>weekday</th>\n",
        "      <th>hour</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "      <td> 9.84</td>\n",
        "      <td> 14.395</td>\n",
        "      <td> 81</td>\n",
        "      <td> 0</td>\n",
        "      <td> 2011</td>\n",
        "      <td> 1</td>\n",
        "      <td> 5</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>1 rows \u00d7 12 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "   season  holiday  workingday  weather  temp   atemp  humidity  windspeed  \\\n",
        "0       1        0           0        1  9.84  14.395        81          0   \n",
        "\n",
        "   year  month  weekday  hour  \n",
        "0  2011      1        5     0  \n",
        "\n",
        "[1 rows x 12 columns]"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Evaluation metric"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#the evaluation metric is the RMSE in the log domain,\n",
      "#so we should transform the target columns into log domain as well. instred of log10(x+1)\n",
      "for col in ['casual', 'registered', 'counts']:\n",
      "    train['log-count'] = train['counts'].apply(lambda x: np.log1p(x))      \n",
      "    train['log-casual'] = train['registered'].apply(lambda x: np.log1p(x))\n",
      "    train['log-registered'] = train['casual'].apply(lambda x: np.log1p(x))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Model development\n",
      "#A first pass "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 200 , 3\n",
      "clf = ensemble.GradientBoostingRegressor(n_estimators=100, max_depth=6)\n",
      "clf.fit(train[features], train['log-count'])\n",
      "result = clf.predict(test[features])\n",
      "result = np.expm1(result)\n",
      " \n",
      "df=pd.DataFrame({'datetime':test['datetime'], 'count':result})\n",
      "df.to_csv('DDDDOOO1.csv', index = False, columns=['datetime','count'])  #Kaggle 0.41661 , 404"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Hyperparameter tuning"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Split data into training and validation sets <16,   >16, 500 ,  4\n",
      "temp = pd.DatetimeIndex(train['datetime'])\n",
      "training = train[temp.day <= 19]\n",
      "validation = train[temp.day > 19]\n",
      " \n",
      "param_grid = {'learning_rate': [0.1, 0.05, 0.01],\n",
      "              'max_depth': [10, 15, 20],\n",
      "              'min_samples_leaf': [3, 5, 10, 20],\n",
      "              }\n",
      " \n",
      "est = ensemble.GradientBoostingRegressor(n_estimators=100)\n",
      "# this may take awhile\n",
      "gs_cv = GridSearchCV(\n",
      "    est, param_grid, n_jobs=3).fit(\n",
      "    training[features], training['log-count'])\n",
      " \n",
      "# best hyperparameter setting\n",
      "gs_cv.best_params_\n",
      " \n",
      "#Baseline error\n",
      "error_count = mean_absolute_error(validation['log-count'], gs_cv.predict(validation[features]))  #???!!!!\n",
      " \n",
      "result = gs_cv.predict(test[features])\n",
      "result = np.expm1(result)\n",
      "df=pd.DataFrame({'datetime':test['datetime'], 'count':result})\n",
      "df.to_csv('DDDDOOO2.csv', index = False, columns=['datetime','count'])  # 19, Kaggle 0.42726, 541\n",
      "#16, Kaggle 0.43659, 726 "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "Found array with 0 sample(s) (shape=(0, 12)) while a minimum of 1 is required.",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-25-86443a79baa1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#Baseline error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0merror_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'log-count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgs_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#???!!!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/aamnah/.local/lib/python2.7/site-packages/sklearn/utils/metaestimators.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/aamnah/.local/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \"\"\"\n\u001b[0;32m--> 387\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'estimator'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/aamnah/.local/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1597\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m         \"\"\"\n\u001b[0;32m-> 1599\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstaged_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/aamnah/.local/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1099\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m         \"\"\"\n\u001b[0;32m-> 1101\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/aamnah/.local/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features)\u001b[0m\n\u001b[1;32m    358\u001b[0m             raise ValueError(\"Found array with %d sample(s) (shape=%s) while a\"\n\u001b[1;32m    359\u001b[0m                              \u001b[0;34m\" minimum of %d is required.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m                              % (n_samples, shape_repr, ensure_min_samples))\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_features\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 12)) while a minimum of 1 is required."
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Tuning the number of estimators     "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "error_train=[]\n",
      "error_validation=[]\n",
      "for k in range(10, 501, 10):\n",
      "    clf = ensemble.GradientBoostingRegressor(n_estimators=k, learning_rate = .05, max_depth = 10,min_samples_leaf = 20)\n",
      " \n",
      "    clf.fit(training[features], training['log-count'])\n",
      "    result = clf.predict(training[features])\n",
      "    error_train.append(mean_absolute_error(result, training['log-count']))\n",
      " \n",
      "    result = clf.predict(validation[features])\n",
      "    error_validation.append(mean_absolute_error(result, validation['log-count']))        \n",
      " \n",
      "#Plot the data\n",
      "x=range(10,501, 10)\n",
      "#plt.style.use('ggplot')\n",
      "#plt.ggplot(x, error_train, 'k')  #plot\n",
      "plt.plot(x, error_validation, 'b')\n",
      "plt.xlabel('Number of Estimators', fontsize=18)\n",
      "plt.ylabel('Error', fontsize=18)\n",
      "plt.legend(['Train', 'Validation'], fontsize=18)\n",
      "plt.title('Error vs. Number of Estimators', fontsize=20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Separate models for registered and casual users"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def merge_predict(model1, model2, test_data):\n",
      "#    Combine the predictions of two separately trained models.\n",
      "#    The input models are in the log domain and returns the predictions\n",
      "#    in original domain.\n",
      "    p1 = np.expm1(model1.predict(test_data))\n",
      "    p2 = np.expm1(model2.predict(test_data))\n",
      "    p_total = (p1+p2)\n",
      "    return(p_total)\n",
      "est_casual = ensemble.GradientBoostingRegressor(n_estimators=11, learning_rate = .01)    # 80, .05\n",
      "est_registered = ensemble.GradientBoostingRegressor(n_estimators=11, learning_rate = .01)\n",
      "param_grid2 = {'max_depth': [10, 15, 20],\n",
      "              'min_samples_leaf': [3, 5, 10, 20],\n",
      "              }\n",
      " \n",
      "gs_casual = GridSearchCV(est_casual, param_grid2, n_jobs=4).fit(training[features], training['log-casual'])\n",
      "gs_registered = GridSearchCV(est_registered, param_grid2, n_jobs=4).fit(training[features], training['log-registered'])      \n",
      " \n",
      "result3 = merge_predict(gs_casual, gs_registered, test[features])\n",
      "df=pd.DataFrame({'datetime':test['datetime'], 'count':result3})\n",
      "df.to_csv('DDDDOOO3.csv', index = False, columns=['datetime','count'])  #Kaggle  1.28286 ,2953"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#<pre>\n",
      "est_casual = ensemble.GradientBoostingRegressor(\n",
      "    n_estimators=80, learning_rate = .05, max_depth = 10,min_samples_leaf = 20)      #80,.05, 10, 20\n",
      "est_registered = ensemble.GradientBoostingRegressor(\n",
      "    n_estimators=80, learning_rate = .05, max_depth = 10,min_samples_leaf = 20)\n",
      " \n",
      "est_casual.fit(train[features].values, train['log-casual'].values)\n",
      "est_registered.fit(train[features].values, train['log-registered'].values)\n",
      "result4 = merge_predict(est_casual, est_registered, test[features])\n",
      " \n",
      "df=pd.DataFrame({'datetime':test['datetime'], 'count':result4})\n",
      "df.to_csv('DDDDOOO4.csv', index = False, columns=['datetime','count']) # Kaggle 0.42163 , 475"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "KeyError",
       "evalue": "\"['month' 'weekday' 'hour'] not in index\"",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-14-a074d2619a58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     n_estimators=80, learning_rate = .05, max_depth = 10,min_samples_leaf = 20)\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mest_casual\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'log-casual'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mest_registered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'log-registered'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mresult4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest_casual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mest_registered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/lib/python2.7/dist-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1653\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/lib/python2.7/dist-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1694\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1695\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1696\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1697\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/lib/python2.7/dist-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m    965\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_setter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'key'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s not in index'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobjarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyError\u001b[0m: \"['month' 'weekday' 'hour'] not in index\""
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Load data and Feature Engineering Different way of defaine columns  without Log "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/env python\n",
      "# -*- coding: utf-8 -*-\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn import linear_model\n",
      "from datetime import datetime\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor,BaggingRegressor\n",
      "import matplotlib.pyplot as plt\n",
      "import csv as csv\n",
      "\n",
      "\n",
      "np.set_printoptions(threshold=np.nan)\n",
      "\n",
      "def transform(df):\n",
      "    i = 0\n",
      "    for timestamp in df['datetime']:\n",
      "        i += 1\n",
      "        date_object = datetime.strptime(timestamp.split()[0], '%Y-%m-%d')\n",
      "        time = timestamp.split()[1][:2]\n",
      "        day = datetime.date(date_object).weekday()\n",
      "        year_dict = {2011:1, 2012:2}\n",
      "        year = year_dict[date_object.year]\n",
      "        df.loc[i-1, 'day'] = day\n",
      "        df.loc[i-1, 'time'] = time\n",
      "        df.loc[i-1, 'year'] = year\n",
      "    return df\n",
      "\n",
      "\n",
      "df_train = pd.read_csv('train.csv')\n",
      "df_test = pd.read_csv('test.csv')\n",
      "train, test = transform(df_train), transform(df_test)\n",
      "\n",
      "cols = ['day', 'time','year','workingday', 'season', 'weather','temp','windspeed']\n",
      "\n",
      "gbr = GradientBoostingRegressor(n_estimators = 100, max_depth=6)\n",
      "\n",
      "casual_gbr = gbr.fit(train[cols], train.casual)\n",
      "\n",
      "predict_casual_gbr = gbr.predict(test[cols])\n",
      "\n",
      "registered_gbr = gbr.fit(train[cols], train.registered)\n",
      "\n",
      "predict_registered_gbr = gbr.predict(test[cols])\n",
      "\n",
      "count = [abs(int(round(i+j))) for i,j in zip(predict_casual_gbr, predict_registered_gbr)]\n",
      "\n",
      "\n",
      "#COUNTS = [(i+j)/2. for i,j in zip(count, count1)]\n",
      "\n",
      "\n",
      "plt.figure();\n",
      "plt.plot(count)\n",
      "##plt.savefig(\"monthly-rental-counts1.png\")\n",
      "plt.xlabel(\"Period 2011-2012\");\n",
      "plt.ylabel(\"Count\")\n",
      "#plt.title(\"Random forest prediction for daily rental counts\");\n",
      "plt.savefig(\"/home/aamnah/Google Drive/Project_BikeSharing/research-project-templete-amnah/Template/images/19RFFFFF.png\")\n",
      "plt.show()\n",
      "\n",
      "df_submission = pd.DataFrame(count, test['datetime'], columns = ['count'])\n",
      "pd.DataFrame.to_csv(df_submission ,'/home/aamnah/Google Drive/Project_BikeSharing/code_and_data/Python_Code/19.csv')\n",
      "\n",
      "#predictions_file = open(\"randomforest_predict_Amnah_17.csv\", \"wb\")\n",
      "#open_file_object = csv.writer(predictions_file)\n",
      "#open_file_object.writerow([\"datetime\",\"count\"])\n",
      "#open_file_object.writerows(zip(df_test, abs(COUNTS))\n",
      "#predictions_file.close()\n",
      "\n",
      "\n",
      "#lm = linear_model.LinearRegression()\n",
      "#casual_lm = lm.fit(train[cols], train.casual)\n",
      "#\n",
      "#predict_casual_lm = lm.predict(test[cols])\n",
      "#\n",
      "#registered = lm.fit(train[cols], train.registered)\n",
      "#\n",
      "#predict_registered_lm = lm.predict(test[cols])\n",
      "#\n",
      "#countlm = [abs(int(round(i+j))) for i,j in zip(predict_casual_lm, predict_registered_lm)]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#rf = RandomForestRegressor(n_estimators=500, min_samples_split=11, oob_score=True)\n",
      "#\n",
      "#casual_rf = rf.fit(train[cols], train.casual)\n",
      "#\n",
      "#predict_casual_rf = rf.predict(test[cols])\n",
      "#\n",
      "#registered = rf.fit(train[cols], train.registered)\n",
      "#\n",
      "#predict_registered_rf = rf.predict(test[cols])\n",
      "#\n",
      "#count = [abs(int(round(i+j))) for i,j in zip(predict_casual_rf, predict_registered_rf)]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import csv as csv\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from datetime import datetime\n",
      "\n",
      "train_df = pd.read_csv('/home/aamnah/Google Drive/Project_BikeSharing/code_and_data/Python_Code/train.csv', header=0)\n",
      "\n",
      "def div6(dt):\n",
      "    return datetime.strptime(dt, \"%Y-%m-%d %H:%M:%S\").time().hour / 6\n",
      "\n",
      "train_df['datetime'] = train_df['datetime'].map(div6)\n",
      "\n",
      "test_df = pd.read_csv('/home/aamnah/Google Drive/Project_BikeSharing/code_and_data/Python_Code/test.csv', header=0)\n",
      "test_dt = test_df['datetime'] # we will need it later\n",
      "test_df['datetime'] = test_df['datetime'].map(div6)\n",
      "\n",
      "# The data is now ready to go. So lets fit to the train, then predict to the test!\n",
      "# Convert back to a numpy array\n",
      "train_data = train_df.values\n",
      "test_data = test_df.values\n",
      "\n",
      "forest = RandomForestClassifier(n_estimators=100)\n",
      "#forest = np.mean(0)\n",
      "forest = forest.fit( train_data[0:1000:,:-3:], train_data[0:1000:,-1])\n",
      "\n",
      "train_predict = forest.predict(train_data[0::,:-3:])\n",
      "\n",
      "# Out predictions for training set\n",
      "train_predict[:10]\n",
      "\n",
      "# Real data\n",
      "train_data[0:10,-1]\n",
      "\n",
      "# Predicting...\n",
      "output = forest.predict(test_data).astype(int)\n",
      "plt.figure();\n",
      "plt.plot(output)\n",
      "plt.savefig(\"RFFFFF1.png\")\n",
      "plt.xlabel(\"period 2011-2012\");\n",
      "plt.ylabel(\"Rentl count\")\n",
      "plt.title(\"Random forest prediction.\");\n",
      "\n",
      "#predictions_file = open(\"my_solution1.csv\", \"wb\")\n",
      "#open_file_object = csv.writer(predictions_file)\n",
      "#open_file_object.writerow([\"datetime\",\"count\"])\n",
      "#open_file_object.writerows(zip(test_dt, output))\n",
      "#predictions_file.close()\n",
      "# Done."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "<matplotlib.text.Text at 0x7fe854b37a50>"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}