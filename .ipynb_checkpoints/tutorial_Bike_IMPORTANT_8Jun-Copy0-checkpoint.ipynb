{
 "metadata": {
  "name": "",
  "signature": "sha256:b067385f6d39247e50f5dcb0139cc9ea3dd3460a289370715380525b51bfd884"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#First let's open Bicycle train.csv and take a look at it. \n",
      "df_train = pd.read_csv('/home/aamnah/Google Drive/Project_BikeSharing/code_and_data/Python_Code/train.csv',parse_dates=[\"datetime\"],header = 0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#In order to see the data we just read in we can use Pandas head command. It allows us to look at any number of lines we need to. I'll go ahead and look at first 100 lines of data. \n",
      "df_train.head(1)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>datetime</th>\n",
        "      <th>season</th>\n",
        "      <th>holiday</th>\n",
        "      <th>workingday</th>\n",
        "      <th>weather</th>\n",
        "      <th>temp</th>\n",
        "      <th>atemp</th>\n",
        "      <th>humidity</th>\n",
        "      <th>windspeed</th>\n",
        "      <th>casual</th>\n",
        "      <th>registered</th>\n",
        "      <th>counts</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>2011-01-01</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "      <td> 9.84</td>\n",
        "      <td> 14.395</td>\n",
        "      <td> 81</td>\n",
        "      <td> 0</td>\n",
        "      <td> 3</td>\n",
        "      <td> 13</td>\n",
        "      <td> 16</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>1 rows \u00d7 12 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "    datetime  season  holiday  workingday  weather  temp   atemp  humidity  \\\n",
        "0 2011-01-01       1        0           0        1  9.84  14.395        81   \n",
        "\n",
        "   windspeed  casual  registered  counts  \n",
        "0          0       3          13      16  \n",
        "\n",
        "[1 rows x 12 columns]"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "#Great! Pandas orginizes everything in a neat table. I wonder what data types we have?\n",
      "df_train.dtypes\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "datetime      datetime64[ns]\n",
        "season                 int64\n",
        "holiday                int64\n",
        "workingday             int64\n",
        "weather                int64\n",
        "temp                 float64\n",
        "atemp                float64\n",
        "humidity               int64\n",
        "windspeed            float64\n",
        "casual                 int64\n",
        "registered             int64\n",
        "counts                 int64\n",
        "dtype: object"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#It would be nice to see how big is our matrix: \n",
      "df_train.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "(10886, 12)"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#So I have 10888 lines and 12 features(columns). I would also like to see if there are any missing values in any of the columns\n",
      "df_train.count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "datetime      10886\n",
        "season        10886\n",
        "holiday       10886\n",
        "workingday    10886\n",
        "weather       10886\n",
        "temp          10886\n",
        "atemp         10886\n",
        "humidity      10886\n",
        "windspeed     10886\n",
        "casual        10886\n",
        "registered    10886\n",
        "counts        10886\n",
        "dtype: int64"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Nice. We don't have anything missing in any column. The only problem for now is the datetime column which is Pandas Series. \n",
      "type(df_train.datetime)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "pandas.core.series.Series"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#To convert a Series or list-like object of date-like objects e.g. strings I can use the to_datetime function. Here I can use pd.DatetimeIndex to extract month, day and hour out of datetime column. For the day column I would like to show it as day of week, where Monday = 0 and Sunday = 6\n",
      "df_train['month'] = pd.DatetimeIndex(df_train.datetime).month\n",
      "df_train['day'] = pd.DatetimeIndex(df_train.datetime).dayofweek\n",
      "df_train['hour'] = pd.DatetimeIndex(df_train.datetime).hour\n",
      "#df_train['year'] = pd.DatetimeIndex(df_train.datetime).year"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_train.head(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>datetime</th>\n",
        "      <th>season</th>\n",
        "      <th>holiday</th>\n",
        "      <th>workingday</th>\n",
        "      <th>weather</th>\n",
        "      <th>temp</th>\n",
        "      <th>atemp</th>\n",
        "      <th>humidity</th>\n",
        "      <th>windspeed</th>\n",
        "      <th>casual</th>\n",
        "      <th>registered</th>\n",
        "      <th>counts</th>\n",
        "      <th>month</th>\n",
        "      <th>day</th>\n",
        "      <th>hour</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>2011-01-01</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "      <td> 9.84</td>\n",
        "      <td> 14.395</td>\n",
        "      <td> 81</td>\n",
        "      <td> 0</td>\n",
        "      <td> 3</td>\n",
        "      <td> 13</td>\n",
        "      <td> 16</td>\n",
        "      <td> 1</td>\n",
        "      <td> 5</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>1 rows \u00d7 15 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "    datetime  season  holiday  workingday  weather  temp   atemp  humidity  \\\n",
        "0 2011-01-01       1        0           0        1  9.84  14.395        81   \n",
        "\n",
        "   windspeed  casual  registered  counts  month  day  hour  \n",
        "0          0       3          13      16      1    5     0  \n",
        "\n",
        "[1 rows x 15 columns]"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Since I got all the information from datetime column, now I will delete it. I will delete casual and registered customer columns as well, to make it more simpler and match test data file.\n",
      "df_train = df_train.drop(['datetime'], axis = 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_train.head(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>season</th>\n",
        "      <th>holiday</th>\n",
        "      <th>workingday</th>\n",
        "      <th>weather</th>\n",
        "      <th>temp</th>\n",
        "      <th>atemp</th>\n",
        "      <th>humidity</th>\n",
        "      <th>windspeed</th>\n",
        "      <th>casual</th>\n",
        "      <th>registered</th>\n",
        "      <th>counts</th>\n",
        "      <th>month</th>\n",
        "      <th>day</th>\n",
        "      <th>hour</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "      <td> 9.84</td>\n",
        "      <td> 14.395</td>\n",
        "      <td> 81</td>\n",
        "      <td> 0</td>\n",
        "      <td> 3</td>\n",
        "      <td> 13</td>\n",
        "      <td> 16</td>\n",
        "      <td> 1</td>\n",
        "      <td> 5</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>1 rows \u00d7 14 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "   season  holiday  workingday  weather  temp   atemp  humidity  windspeed  \\\n",
        "0       1        0           0        1  9.84  14.395        81          0   \n",
        "\n",
        "   casual  registered  counts  month  day  hour  \n",
        "0       3          13      16      1    5     0  \n",
        "\n",
        "[1 rows x 14 columns]"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Ok, good. Let's look at the shape of df_train. \n",
      "df_train.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "(10886, 14)"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#I decided to split our features into two: 1.df_train_target - it is our target. This will contain 'count' column from df_train. \n",
      "#2.df_train_data - this will be our features. All of thte features (of course except 'count') are in here. I also want to use pd.DataFrame.values, which will give us NumPy representation which Scikit-learn can work with. \n",
      "df_train_target = df_train['counts'].values\n",
      "df_train_data = df_train.drop(['counts'],axis = 1).values\n",
      "print 'df_train_data shape is ', df_train_data.shape\n",
      "print 'df_train_target shape is ', df_train_target.shape\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "df_train_data shape is  (10886, 13)\n",
        "df_train_target shape is  (10886,)\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#casual\n",
      "df_train_target1 = df_train['casual'].values\n",
      "df_train_data = df_train.drop(['casual','registered'],axis = 1).values\n",
      "print 'df_train_data shape is ', df_train_data.shape\n",
      "print 'df_train_target shape is ', df_train_target1.shape\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "df_train_data shape is  (10886, 12)\n",
        "df_train_target shape is  (10886,)\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#registered\n",
      "df_train_target2 = df_train['registered'].values\n",
      "df_train_data = df_train.drop(['casual','registered'],axis = 1).values\n",
      "print 'df_train_data shape is ', df_train_data.shape\n",
      "print 'df_train_target shape is ', df_train_target2.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "df_train_data shape is  (10886, 12)\n",
        "df_train_target shape is  (10886,)\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#import pandas as pd                       #4 trys\n",
      "#import numpy as np\n",
      "#import datetime\n",
      "#import csv as csv\n",
      "#import matplotlib.pyplot as plt\n",
      "#from sklearn import datasets, linear_model\n",
      "#from sklearn.preprocessing import OneHotEncoder\n",
      "#from sklearn.ensemble import RandomForestRegressor\n",
      "#from sklearn.ensemble import BaggingRegressor\n",
      "#from sklearn import linear_model\n",
      "#from sklearn import cross_validation\n",
      "#from sklearn import svm\n",
      "#from sklearn.learning_curve import learning_curve\n",
      "#from sklearn.grid_search import GridSearchCV\n",
      "#from sklearn.metrics import explained_variance_score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Machine Learning Algorithm\n",
      "#In this section I will apply some Machine Learning algorithms to our data. \n",
      "#I will also show how to use Grid Search and Cross Validation to improve and find best parameters to the algorithms. \n",
      "from sklearn import linear_model\n",
      "from sklearn import cross_validation\n",
      "from sklearn import svm\n",
      "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor,BaggingRegressor,RandomForestClassifier \n",
      "from sklearn.learning_curve import learning_curve\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.metrics import explained_variance_score\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.ensemble import AdaBoostRegressor\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#222222222222222222222222222222222222222222222Optimization2222222222222222222222222222222222222222222222222222222222222222222"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Here I will use cross-validation, in particular Shuffle Split - random permutation cross-validation that will \n",
      "#give us train and test sets. The test set will be 20% of the original set. I will try Suport Vector Regression,\n",
      "#Ridge Regression and Random Forest Regressor. Run each model three times to see which one gives me the best result. \n",
      "\n",
      "cv = cross_validation.ShuffleSplit(len(df_train_data), n_iter=3, test_size=0.2,random_state=0)\n",
      "#cv = cross_validation.KFold( len(df_train_data), n_folds=3, shuffle=False, random_state=None)   #very bad result\n",
      "\n",
      "\n",
      "print \"LinearRegression\"    \n",
      "for train, test in cv:    \n",
      "   svc = linear_model.LinearRegression().fit(df_train_data[train], df_train_target[train])\n",
      "   print(\"train score: {0:.3f}, test score: {1:.3f}\\n\".format(\n",
      "       svc.score(df_train_data[train], df_train_target[train]), svc.score(df_train_data[test], df_train_target[test])))\n",
      "     "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "LinearRegression\n",
        "train score: 1.000, test score: 1.000\n",
        "\n",
        "train score: 1.000, test score: 1.000\n",
        "\n",
        "train score: 1.000, test score: 1.000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cv = cross_validation.ShuffleSplit(len(df_train_data), n_iter=3, test_size=0.2,\n",
      "    random_state=0)\n",
      "\n",
      "\n",
      "print \"Ridge\"    \n",
      "for train, test in cv:    \n",
      "    svc = linear_model.Ridge().fit(df_train_data[train], df_train_target[train])\n",
      "    print(\"train score: {0:.3f}, test score: {1:.3f}\\n\".format(\n",
      "        svc.score(df_train_data[train], df_train_target[train]), svc.score(df_train_data[test], df_train_target[test])))\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Ridge\n",
        "train score: 1.000, test score: 1.000\n",
        "\n",
        "train score: 1.000, test score: 1.000\n",
        "\n",
        "train score: 1.000, test score: 1.000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"SVR(kernel='rbf',C=10,gamma=.001)\"\n",
      "for train, test in cv:\n",
      "    \n",
      "    svc = svm.SVR(kernel ='rbf', C = 10, gamma = .001).fit(df_train_data[train], df_train_target[train])\n",
      "    print(\"train score: {0:.3f}, test score: {1:.3f}\\n\".format(\n",
      "        svc.score(df_train_data[train], df_train_target[train]), svc.score(df_train_data[test], df_train_target[test])))\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "SVR(kernel='rbf',C=10,gamma=.001)\n",
        "train score: 0.949, test score: 0.945\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "train score: 0.948, test score: 0.953\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "train score: 0.949, test score: 0.939\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Random Forest(n_estimators = 100)\"    \n",
      "for train, test in cv:    \n",
      "    svc1 = RandomForestRegressor(n_estimators = 500).fit(df_train_data[train], df_train_target[train])\n",
      "    print(\"train score: {0:.3f}, test score: {1:.3f}\\n\".format(\n",
      "        svc1.score(df_train_data[train], df_train_target[train]), svc1.score(df_train_data[test], df_train_target[test])))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Random Forest(n_estimators = 100)\n",
        "train score: 1.000, test score: 1.000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "train score: 1.000, test score: 1.000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "train score: 1.000, test score: 1.000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Gradient Boosting Regressor(n_estimators = 100)\"    \n",
      "for train, test in cv:    \n",
      "    svc = GradientBoostingRegressor(n_estimators = 100).fit(df_train_data[train], df_train_target[train])\n",
      "    print(\"train score: {0:.3f}, test score: {1:.3f}\\n\".format(\n",
      "        svc.score(df_train_data[train], df_train_target[train]), svc.score(df_train_data[test], df_train_target[test])))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Gradient Boosting Regressor(n_estimators = 100)\n",
        "train score: 1.000, test score: 1.000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "train score: 1.000, test score: 1.000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "train score: 1.000, test score: 1.000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Bagging Regressor(n_estimators = 100)\"    \n",
      "for train, test in cv:    \n",
      "    svc = BaggingRegressor(n_estimators = 100).fit(df_train_data[train], df_train_target[train])\n",
      "    print(\"train score: {0:.3f}, test score: {1:.3f}\\n\".format(\n",
      "        svc.score(df_train_data[train], df_train_target[train]), svc.score(df_train_data[test], df_train_target[test])))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Bagging Regressor(n_estimators = 100)\n",
        "train score: 1.000, test score: 1.000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "train score: 1.000, test score: 1.000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "train score: 1.000, test score: 1.000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"AdaBoost Regressor(n_estimators = 100)\"    \n",
      "for train, test in cv:    \n",
      "    svc = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4), n_estimators=100).fit(df_train_data[train], df_train_target[train])\n",
      "    print(\"train score: {0:.3f}, test score: {1:.3f}\\n\".format(\n",
      "        svc.score(df_train_data[train], df_train_target[train]), svc.score(df_train_data[test], df_train_target[test])))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "AdaBoost Regressor(n_estimators = 100)\n",
        "train score: 0.999, test score: 0.999\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "train score: 0.999, test score: 0.999\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "train score: 0.999, test score: 0.999\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn.datasets import make_classification\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "\n",
      "\n",
      "#start with casual\n",
      "importances = svc1.feature_importances_\n",
      "\n",
      "important_idx = np.where(importances)[0] \n",
      "\n",
      "important_features = features_list[important_idx]\n",
      "\n",
      "print importances \n",
      "print important_features\n",
      "\n",
      "\n",
      "std = np.std([tree.feature_importances_ for tree in svc.estimators_],\n",
      "             axis=0)\n",
      "indices = np.argsort(importances)[::-1]\n",
      "\n",
      "# Print the feature ranking\n",
      "print(\"Feature ranking:\")\n",
      "#for f in important_features:\n",
      "    #print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
      "#    print  f\n",
      "for f in range(10):\n",
      "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
      "    \n",
      " # Adapted from http://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regression.html\n",
      "pos = np.arange(indices.shape[0]) + .5\n",
      "#plt.subplot(1, 2, 2)\n",
      "plt.barh(pos, importances[important_idx][indices[::-1]], align='center')\n",
      "plt.yticks(pos, important_features[indices[::-1]])\n",
      "plt.xlabel('Relative Importance')\n",
      "plt.title('Variable Importance')\n",
      "plt.draw()\n",
      "plt.savefig(\"/home/aamnah/Google Drive/Project_BikeSharing/research-project-templete-amnah/Template/images/feature.png\")\n",
      "plt.show()   \n",
      "    \n",
      "    \n",
      "    \n",
      "# Plot the feature importances of the forest\n",
      "#plt.figure()\n",
      "#plt.title(\"Feature importances\")\n",
      "#plt.bar(range(10), importances[indices],\n",
      "#       color=\"r\", yerr=std[indices], align=\"center\")\n",
      "#plt.xticks(range(10), indices)\n",
      "#plt.xlim([-1, 10])\n",
      "#plt.savefig(\"/home/aamnah/Google Drive/Project_BikeSharing/research-project-templete-amnah/Template/images/feature.png\")\n",
      "#plt.show()'''\n",
      "\n",
      "###################################I am trying to print name of feature ot index\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'features_list' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-29-e5cdd4ded471>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mimportant_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mimportant_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimportant_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mimportances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'features_list' is not defined"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#I will leave this task to you. Below is the implementation of the Grid Search. \n",
      "#The Grid Search is used for evaluating the best estimator performance score. \n",
      "#I am using train-test-split cross-validation which will give me random train and test subsets. \n",
      "#tuned_paramaters - that's where I set my estimators to be checked. grid_scores_ - returns: \n",
      "#* a dict of parameter settings (in our case tuned_parameters) * the mean score over the cross-validation \n",
      "#folds * the list of scores for each fold best_estimator_ - returns the best estimator chosen by the search \n",
      "\n",
      "X = df_train_data\n",
      "y = df_train_target   #split for 20% test set, 80% train set\n",
      "\n",
      "X_train, X_test, y_train, y_test = cross_validation.train_test_split(\n",
      "    X, y, test_size=0.2, random_state=0)\n",
      "\n",
      "tuned_parameters = [{'n_estimators':[100,500,1000]}]   \n",
      "    \n",
      "scores = ['r2']\n",
      "\n",
      "for score in scores:\n",
      "    \n",
      "    print score\n",
      "    \n",
      "    clf = GridSearchCV(RandomForestRegressor(), tuned_parameters, cv=5, scoring=score)\n",
      "    clf.fit(X_train, y_train)\n",
      "\n",
      "    print(\"Best parameters set found on development set:\")\n",
      "    print \"\"\n",
      "    #best_estimator_ returns the best estimator chosen by the search\n",
      "    print(clf.best_estimator_)\n",
      "    print \"\"\n",
      "    print(\"Grid scores on development set:\")\n",
      "    print \"\"\n",
      "    #grid_scores_ returns:\n",
      "    #    * a dict of parameter settings\n",
      "    #    * the mean score over the cross-validation folds \n",
      "    #    * the list of scores for each fold\n",
      "    for params, mean_score, scores in clf.grid_scores_:\n",
      "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
      "              % (mean_score, scores.std() / 2, params))\n",
      "    print \"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#r2\n",
      "#Best parameters set found on development set:\n",
      "\n",
      "#RandomForestRegressor(bootstrap=True, compute_importances=None,\n",
      " #          criterion='mse', max_depth=None, max_features='auto',\n",
      " #          max_leaf_nodes=None, min_density=None, min_samples_leaf=1,\n",
      " #          min_samples_split=2, n_estimators=500, n_jobs=1,\n",
      " #          oob_score=False, random_state=None, verbose=0)\n",
      "\n",
      "#Grid scores on development set:\n",
      "\n",
      "#0.847 (+/-0.006) for {'n_estimators': 10}\n",
      "#0.863 (+/-0.005) for {'n_estimators': 100}\n",
      "#0.864 (+/-0.005) for {'n_estimators': 500}\n",
      "\n",
      "#This looks good. Now it's ease to pick the estimator that gives us the best result.\n",
      "#Now you can see how Grid Search can be helpful using multiple estimators for other models e.g. SVM.\n",
      "#I would also like to see if the model is high bias or high variance. \n",
      "#Thus I will use learning curve which will graphically display train and cross validation curves. "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
      "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
      "    \n",
      "    plt.figure()\n",
      "    plt.title(title)\n",
      "    if ylim is not None:\n",
      "        plt.ylim(*ylim)\n",
      "    plt.xlabel(\"Training examples\")\n",
      "    plt.ylabel(\"Score\")\n",
      "    train_sizes, train_scores, test_scores = learning_curve(\n",
      "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
      "    train_scores_mean = np.mean(train_scores, axis=1)\n",
      "    train_scores_std = np.std(train_scores, axis=1)\n",
      "    test_scores_mean = np.mean(test_scores, axis=1)\n",
      "    test_scores_std = np.std(test_scores, axis=1)\n",
      "    plt.grid()\n",
      "\n",
      "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
      "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
      "                     color=\"r\")\n",
      "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
      "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
      "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
      "             label=\"Training score\")\n",
      "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
      "             label=\"Cross-validation score\")\n",
      "\n",
      "    plt.legend(loc=\"best\")\n",
      "    return plt\n",
      "\n",
      "\n",
      "title = \"Learning Curves (Random Forest, n_estimators = 500)\"\n",
      "cv = cross_validation.ShuffleSplit(df_train_data.shape[0], n_iter=10,test_size=0.2, random_state=0)\n",
      "estimator = RandomForestRegressor(n_estimators = 500)\n",
      "plot_learning_curve(estimator, title, X, y, (0.0, 1.01), cv=cv, n_jobs=4)\n",
      "#plt.savefig(\"/home/aamnah/Google Drive/Project_BikeSharing/research-project-templete-amnah/Template/images/learn.png\")\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#222222222222222222222222222222222222222222222Optimization2222222222222222222222222222222222222222222222222222222222222222222"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#For optimaze predict each one casula and regested spartil and add thim together\n",
      "#remove atemp\n",
      "#take log10---------------------it gives my bad result\n",
      "#add separet for workingday\n",
      "#take cosine sine for hour seasonalty \n",
      "#try to imporve gbm\n",
      "#paly with cv incease the tarning sets\n",
      "#factorize??????"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_test = pd.read_csv('/home/aamnah/Google Drive/Project_BikeSharing/code_and_data/Python_Code/test.csv',parse_dates=[\"datetime\"],header = 0)\n",
      "\n",
      "#To convert a Series or list-like object of date-like objects e.g. strings I can use the to_datetime function. Here I can use pd.DatetimeIndex to extract month, day and hour out of datetime column. For the day column I would like to show it as day of week, where Monday = 0 and Sunday = 6\n",
      "df_test['month'] = pd.DatetimeIndex(df_test.datetime).month\n",
      "df_test['day'] = pd.DatetimeIndex(df_test.datetime).dayofweek\n",
      "df_test['hour'] = pd.DatetimeIndex(df_test.datetime).hour\n",
      "#df_test['year'] = pd.DatetimeIndex(df_test.datetime).year\n",
      "#Since I got all the information from datetime column, now I will delete it. I will delete casual and registered customer columns as well, to make it more simpler and match test data file.\n",
      "df_test = df_test.drop(['datetime'], axis = 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_test.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_test.head(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Since I got all the information from datetime column, now I will delete it. I will delete casual and registered customer columns as well, to make it more simpler and match test data file.\n",
      "#df_test = df_test.drop(['datetime'], axis = 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#df_test.head(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_test.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_test = pd.read_csv('/home/aamnah/Google Drive/Project_BikeSharing/code_and_data/Python_Code/test.csv',parse_dates=[\"datetime\"],header = 0)\n",
      "df_test['month'] = pd.DatetimeIndex(df_test.datetime).month\n",
      "df_test['day'] = pd.DatetimeIndex(df_test.datetime).dayofweek\n",
      "df_test['hour'] = pd.DatetimeIndex(df_test.datetime).hour\n",
      "df_test = df_test.drop(['datetime'], axis = 1)\n",
      "\n",
      "cv = cross_validation.ShuffleSplit(len(df_train_data), n_iter=3, test_size=0.2,random_state=0)\n",
      "#cv = cross_validation.KFold( len(df_train_data), n_folds=10, shuffle=False, random_state=None)   #very bad result\n",
      "\n",
      "\n",
      "  \n",
      "for train, test in cv: \n",
      "    ml = linear_model.LinearRegression()\n",
      "    r = ml.fit(df_train_data[train], df_train_target[train]) \n",
      "    countln = ml.predict(df_test)\n",
      "\n",
      "df_test = pd.read_csv('/home/aamnah/Google Drive/Project_BikeSharing/code_and_data/Python_Code/test.csv',parse_dates=[\"datetime\"],header = 0)\n",
      "df_submission = pd.DataFrame(abs(countln), df_test[\"datetime\"] , columns = ['count'])\n",
      "pd.DataFrame.to_csv(df_submission ,'Linear_regression_predict_13-0.csv')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_test = pd.read_csv('/home/aamnah/Google Drive/Project_BikeSharing/code_and_data/Python_Code/test.csv',parse_dates=[\"datetime\"],header = 0)\n",
      "df_test['month'] = pd.DatetimeIndex(df_test.datetime).month\n",
      "df_test['day'] = pd.DatetimeIndex(df_test.datetime).dayofweek\n",
      "df_test['hour'] = pd.DatetimeIndex(df_test.datetime).hour\n",
      "df_test = df_test.drop(['datetime'], axis = 1)\n",
      "\n",
      "    \n",
      "cv = cross_validation.ShuffleSplit(len(df_train_data), n_iter=3, test_size=0.2,random_state=0)\n",
      "#cv = cross_validation.KFold( len(df_train_data), n_folds=10, shuffle=False, random_state=None)   #very bad result\n",
      "\n",
      "\n",
      "  \n",
      "for train, test in cv: \n",
      "    svc = linear_model.Ridge()\n",
      "    r = ml.fit(df_train_data[train], df_train_target[train]) \n",
      "    countln = ml.predict(df_test)\n",
      "\n",
      "df_test = pd.read_csv('/home/aamnah/Google Drive/Project_BikeSharing/code_and_data/Python_Code/test.csv',parse_dates=[\"datetime\"],header = 0)\n",
      "df_submission = pd.DataFrame(abs(countln), df_test[\"datetime\"] , columns = ['count'])\n",
      "pd.DataFrame.to_csv(df_submission ,'Linear_regression_Ridge_13.csv')    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_test = pd.read_csv('/home/aamnah/Google Drive/Project_BikeSharing/code_and_data/Python_Code/test.csv',parse_dates=[\"datetime\"],header = 0)\n",
      "df_test['month'] = pd.DatetimeIndex(df_test.datetime).month\n",
      "df_test['day'] = pd.DatetimeIndex(df_test.datetime).dayofweek\n",
      "df_test['hour'] = pd.DatetimeIndex(df_test.datetime).hour\n",
      "df_test = df_test.drop(['datetime'], axis = 1)\n",
      "\n",
      "cv = cross_validation.ShuffleSplit(len(df_train_data), n_iter=3, test_size=0.2,random_state=0)\n",
      "#cv = cross_validation.KFold( len(df_train_data), n_folds=10, shuffle=False, random_state=None)   #very bad result\n",
      "\n",
      "for train, test in cv: \n",
      "    gbr = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4),\n",
      "                          n_estimators=300, random_state=True)\n",
      "    r = gbr.fit(df_train_data[train], df_train_target[train]) \n",
      "    countln = gbr.predict(df_test)\n",
      "\n",
      "df_test = pd.read_csv('/home/aamnah/Google Drive/Project_BikeSharing/code_and_data/Python_Code/test.csv',parse_dates=[\"datetime\"],header = 0)\n",
      "df_submission = pd.DataFrame(abs(countln), df_test[\"datetime\"] , columns = ['count'])\n",
      "pd.DataFrame.to_csv(df_submission ,'AdaBoostRegressor_10-29.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_test['month'] = pd.DatetimeIndex(df_test.datetime).month\n",
      "df_test['day'] = pd.DatetimeIndex(df_test.datetime).dayofweek\n",
      "df_test['hour'] = pd.DatetimeIndex(df_test.datetime).hour\n",
      "df_test = df_test.drop(['datetime'], axis = 1)\n",
      "\n",
      "    \n",
      "cv = cross_validation.ShuffleSplit(len(df_train_data), n_iter=3, test_size=0.2,random_state=0)\n",
      "#cv = cross_validation.KFold( len(df_train_data), n_folds=10, shuffle=False, random_state=None)   #very bad result\n",
      "\n",
      "for train, test in cv: \n",
      "    gbr = BaggingRegressor(n_estimators = 100)\n",
      "    r = gbr.fit(df_train_data[train], df_train_target[train]) \n",
      "    countln = gbr.predict(df_test)\n",
      "\n",
      "df_test = pd.read_csv('/home/aamnah/Google Drive/Project_BikeSharing/code_and_data/Python_Code/test.csv',parse_dates=[\"datetime\"],header = 0)\n",
      "df_submission = pd.DataFrame(abs(countln), df_test[\"datetime\"] , columns = ['count'])\n",
      "pd.DataFrame.to_csv(df_submission ,'BaggingRegressor_BR_10-29.csv')\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_test['month'] = pd.DatetimeIndex(df_test.datetime).month\n",
      "df_test['day'] = pd.DatetimeIndex(df_test.datetime).dayofweek\n",
      "df_test['hour'] = pd.DatetimeIndex(df_test.datetime).hour\n",
      "df_test = df_test.drop(['datetime'], axis = 1)\n",
      "\n",
      "\n",
      "cv = cross_validation.ShuffleSplit(len(df_train_data), n_iter=3, test_size=0.2,random_state=0)\n",
      "#cv = cross_validation.KFold( len(df_train_data), n_folds=10, shuffle=False, random_state=None)   #very bad result\n",
      "\n",
      "for train, test in cv: \n",
      "    gbr = GradientBoostingRegressor(n_estimators = 100, max_depth=6)\n",
      "    r = gbr.fit(df_train_data[train], df_train_target[train]) \n",
      "    countln = gbr.predict(df_test)\n",
      "\n",
      "df_test = pd.read_csv('/home/aamnah/Google Drive/Project_BikeSharing/code_and_data/Python_Code/test.csv',parse_dates=[\"datetime\"],header = 0)\n",
      "df_submission = pd.DataFrame(abs(countln), df_test[\"datetime\"] , columns = ['count'])\n",
      "pd.DataFrame.to_csv(df_submission ,'GradientBoostingRegressor_GBR_10-29.csv')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#rf = RandomForestRegressor(n_estimators=1000, min_samples_split=6, oob_score=True)\n",
      "#svc = RandomForestRegressor(n_estimators = 100).fit(df_train_data[train], df_train_target[train])\n",
      "#This the best result based on the Grid search outcome\n",
      "cv = cross_validation.ShuffleSplit(len(df_train_data), n_iter=3, test_size=0.2,random_state=0)\n",
      "\n",
      "for train, test in cv: \n",
      "    gbr = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=1000, n_jobs=1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "    r = gbr.fit(df_train_data[train], df_train_target[train]) \n",
      "    countln = gbr.predict(df_test)\n",
      "\n",
      "\n",
      "df_test = pd.read_csv('/home/aamnah/Google Drive/Project_BikeSharing/code_and_data/Python_Code/test.csv',parse_dates=[\"datetime\"],header = 0)\n",
      "df_submission = pd.DataFrame(abs(countln), df_test[\"datetime\"] , columns = ['count'])\n",
      "pd.DataFrame.to_csv(df_submission ,'randomforest_13-2.csv')\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$Stage TWO SEPARATE $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$#"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#First let's open Bicycle train.csv and take a look at it. \n",
      "df_train = pd.read_csv('/home/aamnah/Google Drive/Project_BikeSharing/code_and_data/Python_Code/train.csv',parse_dates=[\"datetime\"],header = 0)\n",
      "\n",
      "#To convert a Series or list-like object of date-like objects e.g. strings I can use the to_datetime function. Here I can use pd.DatetimeIndex to extract month, day and hour out of datetime column. For the day column I would like to show it as day of week, where Monday = 0 and Sunday = 6\n",
      "df_train['month'] = pd.DatetimeIndex(df_train.datetime).month\n",
      "df_train['day'] = pd.DatetimeIndex(df_train.datetime).dayofweek\n",
      "df_train['hour'] = pd.DatetimeIndex(df_train.datetime).hour\n",
      "\n",
      "#Since I got all the information from datetime column, now I will delete it. I will delete casual and registered customer columns as well, to make it more simpler and match test data file.\n",
      "df_train = df_train.drop(['datetime'], axis = 1)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_test = pd.read_csv('/home/aamnah/Google Drive/Project_BikeSharing/code_and_data/Python_Code/test.csv',parse_dates=[\"datetime\"],header = 0)\n",
      "\n",
      "#To convert a Series or list-like object of date-like objects e.g. strings I can use the to_datetime function. Here I can use pd.DatetimeIndex to extract month, day and hour out of datetime column. For the day column I would like to show it as day of week, where Monday = 0 and Sunday = 6\n",
      "df_test['month'] = pd.DatetimeIndex(df_test.datetime).month\n",
      "df_test['day'] = pd.DatetimeIndex(df_test.datetime).dayofweek\n",
      "df_test['hour'] = pd.DatetimeIndex(df_test.datetime).hour\n",
      "\n",
      "#Since I got all the information from datetime column, now I will delete it. I will delete casual and registered customer columns as well, to make it more simpler and match test data file.\n",
      "df_test = df_test.drop(['datetime'], axis = 1)\n",
      "df_test.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_train_target1 = df_train['casual'].values\n",
      "df_train_data1 = df_train.drop(['casual','registered','counts'],axis = 1).values\n",
      "print 'df_train_data shape is ', df_train_data1.shape\n",
      "print 'df_train_target shape is ', df_train_target1.shape\n",
      "#registered\n",
      "df_train_target2 = df_train['registered'].values\n",
      "df_train_data2 = df_train.drop(['registered','casual','counts'],axis = 1).values\n",
      "print 'df_train_data shape is ', df_train_data2.shape\n",
      "print 'df_train_target shape is ', df_train_target2.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cv = cross_validation.ShuffleSplit(len(df_train_data), n_iter=3, test_size=0.2,random_state=0)\n",
      "\n",
      "\n",
      "for train, test in cv: \n",
      "    \n",
      "    lm = linear_model.LinearRegression()\n",
      "\n",
      "    casual_lm = lm.fit(df_train_data1[train], df_train_target1[train])\n",
      "    predict_casual_lm = lm.predict(df_test)\n",
      "\n",
      "    registered = lm.fit(df_train_data2[train], df_train_target2[train])\n",
      "    predict_registered_lm = lm.predict(df_test)\n",
      "\n",
      "    \n",
      "countlm = [abs(int(round(i+j))) for i,j in zip(predict_casual_lm, predict_registered_lm)]\n",
      "\n",
      "plt.figure();\n",
      "plt.plot(countlm)\n",
      "##plt.savefig(\"monthly-rental-counts1.png\")\n",
      "plt.xlabel(\"Period 2011-2012\");\n",
      "plt.ylabel(\"Count\")\n",
      "#plt.title(\"Random forest prediction for daily rental counts\");\n",
      "plt.savefig(\"/home/aamnah/Google Drive/Project_BikeSharing/research-project-templete-amnah/Template/images/LinearRegression_2.png\")\n",
      "plt.show()\n",
      "\n",
      "df_test = pd.read_csv('/home/aamnah/Google Drive/Project_BikeSharing/code_and_data/Python_Code/test.csv',parse_dates=[\"datetime\"],header = 0)\n",
      "df_submission = pd.DataFrame(countlm, df_test['datetime'], columns = ['count'])\n",
      "pd.DataFrame.to_csv(df_submission ,'/home/aamnah/Google Drive/Project_BikeSharing/code_and_data/Python_Code/LinearRegression_2.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cv = cross_validation.ShuffleSplit(len(df_train_data), n_iter=3, test_size=0.2,random_state=0)\n",
      "\n",
      "\n",
      "for train, test in cv: \n",
      "    \n",
      "    lm = linear_model.Ridge()\n",
      "\n",
      "    casual_lm = lm.fit(df_train_data1[train], df_train_target1[train])\n",
      "    predict_casual_lm = lm.predict(df_test)\n",
      "\n",
      "    registered = lm.fit(df_train_data2[train], df_train_target2[train])\n",
      "    predict_registered_lm = lm.predict(df_test)\n",
      "\n",
      "    \n",
      "countlm = [abs(int(round(i+j))) for i,j in zip(predict_casual_lm, predict_registered_lm)]\n",
      "\n",
      "plt.figure();\n",
      "plt.plot(countlm)\n",
      "##plt.savefig(\"monthly-rental-counts1.png\")\n",
      "plt.xlabel(\"Period 2011-2012\");\n",
      "plt.ylabel(\"Count\")\n",
      "#plt.title(\"Random forest prediction for daily rental counts\");\n",
      "plt.savefig(\"/home/aamnah/Google Drive/Project_BikeSharing/research-project-templete-amnah/Template/images/Ridge_2.png\")\n",
      "plt.show()\n",
      "\n",
      "df_test = pd.read_csv('/home/aamnah/Google Drive/Project_BikeSharing/code_and_data/Python_Code/test.csv',parse_dates=[\"datetime\"],header = 0)\n",
      "df_submission = pd.DataFrame(countlm, df_test['datetime'], columns = ['count'])\n",
      "pd.DataFrame.to_csv(df_submission ,'/home/aamnah/Google Drive/Project_BikeSharing/code_and_data/Python_Code/Ridge_2.csv')\n",
      "\n",
      "\n",
      "\n",
      "df_test['month'] = pd.DatetimeIndex(df_test.datetime).month\n",
      "df_test['day'] = pd.DatetimeIndex(df_test.datetime).dayofweek\n",
      "df_test['hour'] = pd.DatetimeIndex(df_test.datetime).hour\n",
      "\n",
      "#Since I got all the information from datetime column, now I will delete it. I will delete casual and registered customer columns as well, to make it more simpler and match test data file.\n",
      "df_test = df_test.drop(['datetime'], axis = 1)\n",
      "df_test.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cv = cross_validation.ShuffleSplit(len(df_train_data), n_iter=3, test_size=0.2,random_state=0)\n",
      "\n",
      "\n",
      "for train, test in cv: \n",
      "    \n",
      "    lm = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4),\n",
      "                          n_estimators=300, random_state=True)\n",
      "\n",
      "    casual_lm = lm.fit(df_train_data1[train], df_train_target1[train])\n",
      "    predict_casual_lm = lm.predict(df_test)\n",
      "\n",
      "    registered = lm.fit(df_train_data2[train], df_train_target2[train])\n",
      "    predict_registered_lm = lm.predict(df_test)\n",
      "\n",
      "    \n",
      "countlm = [abs(int(round(i+j))) for i,j in zip(predict_casual_lm, predict_registered_lm)]\n",
      "\n",
      "plt.figure();\n",
      "plt.plot(countlm)\n",
      "##plt.savefig(\"monthly-rental-counts1.png\")\n",
      "plt.xlabel(\"Period 2011-2012\");\n",
      "plt.ylabel(\"Count\")\n",
      "#plt.title(\"Random forest prediction for daily rental counts\");\n",
      "plt.savefig(\"/home/aamnah/Google Drive/Project_BikeSharing/research-project-templete-amnah/Template/images/AdaBoostRegressor_2.png\")\n",
      "plt.show()\n",
      "\n",
      "df_test = pd.read_csv('/home/aamnah/Google Drive/Project_BikeSharing/code_and_data/Python_Code/test.csv',parse_dates=[\"datetime\"],header = 0)\n",
      "df_submission = pd.DataFrame(countlm, df_test['datetime'], columns = ['count'])\n",
      "pd.DataFrame.to_csv(df_submission ,'/home/aamnah/Google Drive/Project_BikeSharing/code_and_data/Python_Code/AdaBoostRegressor_2.csv')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "df_test['month'] = pd.DatetimeIndex(df_test.datetime).month\n",
      "df_test['day'] = pd.DatetimeIndex(df_test.datetime).dayofweek\n",
      "df_test['hour'] = pd.DatetimeIndex(df_test.datetime).hour\n",
      "\n",
      "#Since I got all the information from datetime column, now I will delete it. I will delete casual and registered customer columns as well, to make it more simpler and match test data file.\n",
      "df_test = df_test.drop(['datetime'], axis = 1)\n",
      "df_test.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cv = cross_validation.ShuffleSplit(len(df_train_data), n_iter=3, test_size=0.2,random_state=0)\n",
      "\n",
      "\n",
      "for train, test in cv: \n",
      "    \n",
      "    lm = BaggingRegressor(n_estimators = 100)\n",
      "\n",
      "    casual_lm = lm.fit(df_train_data1[train], df_train_target1[train])\n",
      "    predict_casual_lm = lm.predict(df_test)\n",
      "\n",
      "    registered = lm.fit(df_train_data2[train], df_train_target2[train])\n",
      "    predict_registered_lm = lm.predict(df_test)\n",
      "\n",
      "    \n",
      "countlm = [abs(int(round(i+j))) for i,j in zip(predict_casual_lm, predict_registered_lm)]\n",
      "\n",
      "plt.figure();\n",
      "plt.plot(countlm)\n",
      "##plt.savefig(\"monthly-rental-counts1.png\")\n",
      "plt.xlabel(\"Period 2011-2012\");\n",
      "plt.ylabel(\"Count\")\n",
      "#plt.title(\"Random forest prediction for daily rental counts\");\n",
      "plt.savefig(\"/home/aamnah/Google Drive/Project_BikeSharing/research-project-templete-amnah/Template/images/BaggingRegressor_2.png\")\n",
      "plt.show()\n",
      "\n",
      "df_test = pd.read_csv('/home/aamnah/Google Drive/Project_BikeSharing/code_and_data/Python_Code/test.csv',parse_dates=[\"datetime\"],header = 0)\n",
      "df_submission = pd.DataFrame(countlm, df_test['datetime'], columns = ['count'])\n",
      "pd.DataFrame.to_csv(df_submission ,'/home/aamnah/Google Drive/Project_BikeSharing/code_and_data/Python_Code/BaggingRegressor_2.csv')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cv = cross_validation.ShuffleSplit(len(df_train_data), n_iter=3, test_size=0.2,random_state=0)\n",
      "\n",
      "\n",
      "for train, test in cv: \n",
      "    \n",
      "    lm = GradientBoostingRegressor(n_estimators = 100, max_depth=6)\n",
      "\n",
      "    casual_lm = lm.fit(df_train_data1[train], df_train_target1[train])\n",
      "    predict_casual_lm = lm.predict(df_test)\n",
      "\n",
      "    registered = lm.fit(df_train_data2[train], df_train_target2[train])\n",
      "    predict_registered_lm = lm.predict(df_test)\n",
      "\n",
      "    \n",
      "countlm = [abs(int(round(i+j))) for i,j in zip(predict_casual_lm, predict_registered_lm)]\n",
      "\n",
      "plt.figure();\n",
      "plt.plot(countlm)\n",
      "##plt.savefig(\"monthly-rental-counts1.png\")\n",
      "plt.xlabel(\"Period 2011-2012\");\n",
      "plt.ylabel(\"Count\")\n",
      "#plt.title(\"Random forest prediction for daily rental counts\");\n",
      "plt.savefig(\"/home/aamnah/Google Drive/Project_BikeSharing/research-project-templete-amnah/Template/images/GradientBoostingRegressor_2.png\")\n",
      "plt.show()\n",
      "\n",
      "df_test = pd.read_csv('/home/aamnah/Google Drive/Project_BikeSharing/code_and_data/Python_Code/test.csv',parse_dates=[\"datetime\"],header = 0)\n",
      "df_submission = pd.DataFrame(countlm, df_test['datetime'], columns = ['count'])\n",
      "pd.DataFrame.to_csv(df_submission ,'/home/aamnah/Google Drive/Project_BikeSharing/code_and_data/Python_Code/GradientBoostingRegressor_2.csv')\n",
      "\n",
      "\n",
      "\n",
      "df_test['month'] = pd.DatetimeIndex(df_test.datetime).month\n",
      "df_test['day'] = pd.DatetimeIndex(df_test.datetime).dayofweek\n",
      "df_test['hour'] = pd.DatetimeIndex(df_test.datetime).hour\n",
      "\n",
      "#Since I got all the information from datetime column, now I will delete it. I will delete casual and registered customer columns as well, to make it more simpler and match test data file.\n",
      "df_test = df_test.drop(['datetime'], axis = 1)\n",
      "df_test.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cv = cross_validation.ShuffleSplit(len(df_train_data), n_iter=3, test_size=0.2,random_state=0)\n",
      "\n",
      "\n",
      "for train, test in cv: \n",
      "    \n",
      "    lm = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=1000, n_jobs=1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "    casual_lm = lm.fit(df_train_data1[train], df_train_target1[train])\n",
      "    predict_casual_lm = lm.predict(df_test)\n",
      "\n",
      "    registered = lm.fit(df_train_data2[train], df_train_target2[train])\n",
      "    predict_registered_lm = lm.predict(df_test)\n",
      "\n",
      "    \n",
      "countlm = [abs(int(round(i+j))) for i,j in zip(predict_casual_lm, predict_registered_lm)]\n",
      "\n",
      "plt.figure();\n",
      "plt.plot(countlm)\n",
      "##plt.savefig(\"monthly-rental-counts1.png\")\n",
      "plt.xlabel(\"Period 2011-2012\");\n",
      "plt.ylabel(\"Count\")\n",
      "#plt.title(\"Random forest prediction for daily rental counts\");\n",
      "plt.savefig(\"/home/aamnah/Google Drive/Project_BikeSharing/research-project-templete-amnah/Template/images/RandomForestRegressor_2.png\")\n",
      "plt.show()\n",
      "\n",
      "df_test = pd.read_csv('/home/aamnah/Google Drive/Project_BikeSharing/code_and_data/Python_Code/test.csv',parse_dates=[\"datetime\"],header = 0)\n",
      "df_submission = pd.DataFrame(countlm, df_test['datetime'], columns = ['count'])\n",
      "pd.DataFrame.to_csv(df_submission ,'/home/aamnah/Google Drive/Project_BikeSharing/code_and_data/Python_Code/RandomForestRegressor_2.csv')\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}